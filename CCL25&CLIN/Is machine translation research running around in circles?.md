# Is machine translation research running around in circles?

## Machine translation everywhere

Two main uses

* Dissemination: professional translators use as a starting point
* Assimilation (also gisting): anyone can use MT as is to make sense of text in a foreign language

Gisting is the main use of MT!

Usefulness as a goal. Requires evaluation of usefulness, or estimates if evaluation not possible.

Usefulness depends on the task.

## Measuring usefulness

Some things are not measurable:

* News
* Literature
* Marketing and advertising

Perception of productivity is not the same as measured productivity! (Yoast: How can we measure the quality of the Spanish prominent words? (Insert other readability/SEO check) Without judging.)

## The need for automatic evaluation

Human in the loop error rate.

BLEU, NIST, METEOR. We hope that this will increase usefulness.

## Running around in circles?

In spite of all this, research still sticks massively to BLEU.
As a result, we do not really know whether the usefulness of MT is improving.

Most **machine translation research** may be **running around in circles!**

## So what can we do?

Why _hope_ when you could _tune_?

Research scarce on how readers make sense of MT output:

* Similar to non-native version of their language?
* Is raw MT similar to a contact language (pidgin or creole) based on the target language?
* Is it similar to understanding a closely-related foreign language?
* Do readers adapt if a language is close (Dutch-German)

[ Insert photo of slide here ]

## Concluding remarks

Most MT research is driven by automatic evaluation metrics such as BLEU.
